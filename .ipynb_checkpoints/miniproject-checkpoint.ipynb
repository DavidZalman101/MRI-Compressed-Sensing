{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73c73da-124b-44a5-9484-a0947755bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from models.vanilla import VanillaModel\n",
    "from torch import optim, nn\n",
    "from data.mri_dataset import SliceData\n",
    "from torch.utils.data import DataLoader\n",
    "from data.mri_dataset import DataTransform\n",
    "from data import transforms\n",
    "from models.subsampling import SubsamplingLayer\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8355147d-1cb1-4e4f-927c-45d685b4a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952e9aa0-c919-40ed-8983-d010d9c4702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, device, mask_lr, results_root, drop_rate, learn_mask=False):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.mask_lr = mask_lr\n",
    "        self.results_root = results_root\n",
    "        self.early_stopping_patience = 5\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.no_improve_epochs = 0\n",
    "        self.best_model_state = None\n",
    "        self.learn_mask = learn_mask\n",
    "        self.drop_rate = drop_rate\n",
    "        self.train_psnr_mean = 0\n",
    "        self.train_psnr_std = 0\n",
    "        self.test_psnr_mean = 0\n",
    "        self.test_psnr_std = 0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "    def fit(self, train_loader, val_loader, epochs, i = 0):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_psnr, train_psnr_std = self.train_epoch(train_loader)\n",
    "            val_loss, val_psnr, val_psnr_std = self.evaluate(val_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            print(f'Epopch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}, Train PSNR: {train_psnr}, Val PSNR: {val_psnr} ')\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                self.no_improve_epochs = 0\n",
    "            else:\n",
    "                self.no_improve_epochs += 1\n",
    "                if self.no_improve_epochs >= self.early_stopping_patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    break\n",
    "\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "        self.train_psnr_mean, self.train_psnr_std = train_psnr, train_psnr_std\n",
    "        self.plot_losses(i)\n",
    "\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        psnr_values = []\n",
    "        \n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.model.subsample.learn_mask:\n",
    "                self.model.subsample.mask_grad(self.mask_lr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            psnr_values.append(self.calculate_psnr(outputs, targets))\n",
    "\n",
    "            del inputs, outputs, targets, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        psnr_std = np.std(psnr_values)\n",
    "        return avg_loss, avg_psnr, psnr_std\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        psnr_values = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                psnr_values.append(self.calculate_psnr(outputs, targets))\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        psnr_std = np.std(psnr_values)\n",
    "        return avg_loss, avg_psnr, psnr_std\n",
    "\n",
    "    def calculate_psnr(self, output, target):\n",
    "        mse = torch.mean((output - target) ** 2)\n",
    "        if mse == 0:\n",
    "            return float('inf')\n",
    "        max_pixel_value = torch.max(target) - torch.min(target)  # Updated data range calculation\n",
    "        psnr = 20 * torch.log10(max_pixel_value / torch.sqrt(mse))\n",
    "        return psnr.item()\n",
    "\n",
    "    def save_psnr_results(self):\n",
    "            os.makedirs(f'{self.results_root}/psnr', exist_ok=True)\n",
    "            mask_status = \"learned_mask\" if self.learn_mask else \"unlearned_mask\"\n",
    "            with open(f'{self.results_root}/psnr/{mask_status}_{self.drop_rate}.txt', 'w') as f:\n",
    "                f.write(f'Train PSNR mean: {self.train_psnr_mean}\\n')\n",
    "                f.write(f'Train PSNR std: {self.train_psnr_std}\\n')\n",
    "                f.write(f'Test PSNR mean: {self.test_psnr_mean}\\n')\n",
    "                f.write(f'Test PSNR std: {self.test_psnr_std}\\n')\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        _, self.test_psnr_mean, self.test_psnr_std = self.evaluate(test_loader)\n",
    "        self.save_images(test_loader)\n",
    "\n",
    "    def save_images(self, test_loader):\n",
    "        self.model.eval()\n",
    "        freq, image = next(iter(test_loader))\n",
    "        output = self.model(freq.to(self.device)).squeeze(1)\n",
    "\n",
    "        os.makedirs(f'{self.results_root}/images', exist_ok=True)\n",
    "        mask_status = \"learned_mask\" if self.learn_mask else \"unlearned_mask\"\n",
    "        \n",
    "        # Save output image\n",
    "        plt.imshow(output[0].detach().cpu().numpy(), cmap='gray')\n",
    "        plt.savefig(f'{self.results_root}/images/{mask_status}_output_{self.drop_rate}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save target image\n",
    "        plt.imshow(image[0].detach().cpu().numpy(), cmap='gray')\n",
    "        plt.savefig(f'{self.results_root}/images/{mask_status}_true_{self.drop_rate}.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_losses(self, i = 0):\n",
    "        os.makedirs(f'{self.results_root}/graphs', exist_ok=True)\n",
    "        mask_status = \"learned_mask\" if self.learn_mask else \"unlearned_mask\"\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'{self.results_root}/graphs/{mask_status}_loss_graph_{self.drop_rate}_{i}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1e9ea7-6e28-41c0-a810-c6e9b543c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class to mimic the argparser above\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed = 0\n",
    "        self.data_path = '/datasets/fastmri_knee/'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.batch_size = 16\n",
    "        self.num_workers = 1\n",
    "        self.num_epochs = 6\n",
    "        self.report_interval = 10\n",
    "        self.drop_rate = 0.1\n",
    "        self.learn_mask = False\n",
    "        self.results_root = 'results'\n",
    "        self.lr =  0.001\n",
    "        self.mask_lr = 0.001\n",
    "        self.val_test_split = 0.3\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da8ebd8-71c6-42f5-a13c-cb62685c6453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def create_datasets(args,resolution=320):\n",
    "    '''This function creates the train and test datasets.\n",
    "    You probably wouldn't need to change it'''\n",
    "    \n",
    "    train_data = SliceData(\n",
    "        root=f\"{args.data_path}/singlecoil_train\",\n",
    "        transform=DataTransform(resolution),\n",
    "        split=1\n",
    "    )\n",
    "    dev_data = SliceData(\n",
    "        root=f\"{args.data_path}/singlecoil_val\",\n",
    "        transform=DataTransform(resolution),\n",
    "        split = args.val_test_split,\n",
    "        validation = True\n",
    "    )\n",
    "    test_data = SliceData(\n",
    "        root=f\"{args.data_path}/singlecoil_val\",\n",
    "        transform=DataTransform(resolution),\n",
    "        split = args.val_test_split,\n",
    "        validation = False\n",
    "    )\n",
    "\n",
    "    data_size = 50\n",
    "    train_data.examples = train_data.examples[:data_size]\n",
    "    dev_data.examples = dev_data.examples[:int((data_size*args.val_test_split)//4)]\n",
    "    test_data.examples = test_data.examples[:int(data_size*(1-args.val_test_split)//4)]\n",
    "\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "def create_data_loaders(args):\n",
    "    '''Create train, validation and test datasets, and then out of them create the dataloaders. \n",
    "       These loaders will automatically apply needed transforms, as dictated in the create_datasets function using the transform parameter.'''\n",
    "    train_data, dev_data, test_data = create_datasets(args)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, dev_loader, test_loader\n",
    "\n",
    "def freq_to_image(freq_data):\n",
    "    ''' \n",
    "    This function accepts as input an image in the frequency domain, of size (B,320,320,2) (where B is batch size).\n",
    "    Returns a tensor of size (B,320,320) representing the data in image domain.\n",
    "    '''\n",
    "    return transforms.complex_abs(transforms.ifft2_regular(freq_data))\n",
    "\n",
    "\n",
    "train_loader, validation_loader, test_loader = create_data_loaders(args) #get dataloaders\n",
    "print(len(train_loader.dataset))\n",
    "print(len(validation_loader.dataset))\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598edeac-7bd9-4160-aeef-a1c4e8b35ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):  # Added dropout probability\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\" A U-Net model that uses residual blocks and includes a self-attention mechanism \"\"\"\n",
    "    def __init__(self, drop_rate, device, learn_mask, num_channels=32, pool_kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.subsample = SubsamplingLayer(drop_rate, device, learn_mask)\n",
    "        self.down1 = ResidualBlock(1, num_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.down2 = ResidualBlock(num_channels, num_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.bottleneck = ResidualBlock(num_channels * 2, num_channels * 4)\n",
    "        self.up1 = nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=2, stride=2)\n",
    "        self.up_block1 = ResidualBlock(num_channels * 4, num_channels * 2)\n",
    "        self.up2 = nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=2, stride=2)\n",
    "        self.up_block2 = ResidualBlock(num_channels * 2, num_channels)\n",
    "        self.final_conv = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.subsample(x)\n",
    "        #Downsampling\n",
    "        x1 = self.down1(x)\n",
    "        x = self.pool1(x1)\n",
    "        x2 = self.down2(x)\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        # Bottelneck and attention\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Upsampling\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.up_block1(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.up_block2(x)\n",
    "        \n",
    "        return self.final_conv(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b3962d-330b-41fc-9972-12dc70a6071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Epopch: 0, Train Loss: 0.4953901022672653, Val Loss: 0.6425914168357849, Train PSNR: 21.094398975372314, Val PSNR: 17.198169708251953 \n",
      "Epopch: 1, Train Loss: 0.23383933678269386, Val Loss: 0.4622328281402588, Train PSNR: 24.044400215148926, Val PSNR: 18.628910064697266 \n",
      "Epopch: 2, Train Loss: 0.24694739654660225, Val Loss: 0.4198969006538391, Train PSNR: 23.055622577667236, Val PSNR: 19.046091079711914 \n",
      "Epopch: 3, Train Loss: 0.2508646883070469, Val Loss: 0.26970407366752625, Train PSNR: 23.165512084960938, Val PSNR: 20.968643188476562 \n",
      "Epopch: 4, Train Loss: 0.19921371713280678, Val Loss: 0.20227639377117157, Train PSNR: 24.62506866455078, Val PSNR: 22.21806526184082 \n",
      "Epopch: 5, Train Loss: 0.22034508734941483, Val Loss: 0.18953582644462585, Train PSNR: 23.66673231124878, Val PSNR: 22.50060272216797 \n"
     ]
    }
   ],
   "source": [
    "model = UNetModel(args.drop_rate, args.device, args.learn_mask).to(args.device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "trainer = Trainer(model, optimizer, loss_fn, args.device, args.mask_lr, args.results_root, args.drop_rate)\n",
    "\n",
    "print(\"Starting\")\n",
    "trainer.fit(train_loader, validation_loader, args.num_epochs, 0)\n",
    "trainer.test(test_loader)\n",
    "trainer.save_psnr_results()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94cecf1-86ed-4117-82ee-c6d05211060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):  # Added dropout probability\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\" A U-Net model that uses residual blocks and includes a self-attention mechanism \"\"\"\n",
    "    def __init__(self, drop_rate, device, learn_mask, num_channels=32, pool_kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.subsample = SubsamplingLayer(drop_rate, device, learn_mask)\n",
    "        self.down1 = ResidualBlock(1, num_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.down2 = ResidualBlock(num_channels, num_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.bottleneck = ResidualBlock(num_channels * 2, num_channels * 4)\n",
    "        self.up1 = nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=2, stride=2)\n",
    "        self.up_block1 = ResidualBlock(num_channels * 4, num_channels * 2)\n",
    "        self.up2 = nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=2, stride=2)\n",
    "        self.up_block2 = ResidualBlock(num_channels * 2, num_channels)\n",
    "        self.final_conv = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.subsample(x)\n",
    "        #Downsampling\n",
    "        x1 = self.down1(x)\n",
    "        x = self.pool1(x1)\n",
    "        x2 = self.down2(x)\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        # Bottelneck and attention\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Upsampling\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.up_block1(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.up_block2(x)\n",
    "        \n",
    "        return self.final_conv(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e360d4-42ce-4c97-992d-20383915f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, drop_prob = 0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\" A U-Net model that uses residual blocks and includes a self-attention mechanism \"\"\"\n",
    "    def __init__(self, drop_rate, device, learn_mask, num_channels=32, pool_kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.subsample = SubsamplingLayer(drop_rate, device, learn_mask)\n",
    "        self.down1 = ResidualBlock(1, num_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.down2 = ResidualBlock(num_channels, num_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.bottleneck = ResidualBlock(num_channels * 2, num_channels * 4)\n",
    "        self.up1 = nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=2, stride=2)\n",
    "        self.up_block1 = ResidualBlock(num_channels * 4, num_channels * 2)\n",
    "        self.up2 = nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=2, stride=2)\n",
    "        self.up_block2 = ResidualBlock(num_channels * 2, num_channels)\n",
    "        self.final_conv = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.subsample(x)\n",
    "        #Downsampling\n",
    "        x1 = self.down1(x)\n",
    "        x = self.pool1(x1)\n",
    "        x2 = self.down2(x)\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        # Bottelneck and attention\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Upsampling\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.up_block1(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.up_block2(x)\n",
    "        \n",
    "        return self.final_conv(x).squeeze(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
